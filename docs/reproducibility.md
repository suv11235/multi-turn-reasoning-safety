# Reproducibility Checklist

1. **Environment**
   - Python >= 3.11
   - Install dependencies: `pip install -r requirements.txt` (to be generated by scripts headers).
   - GPU optional; CPU inference supported for smaller batches.

2. **Configuration**
   - All inference parameters stored in `datasets/inference_config.yaml`.
   - Prompt metadata canonicalized in `prompts/prompts_refusal_benchmark.jsonl`.

3. **Workflow**
   1. `python scripts/run_baseline.py --config datasets/inference_config.yaml --prompts prompts/prompts_refusal_benchmark.jsonl --out results/generations/deepseek_distill_llama31_run1.jsonl`
   2. `python scripts/auto_label_refusals.py --inputs results/generations/deepseek_distill_llama31_run1.jsonl --out results/metrics/auto_labels_run1.jsonl`
   3. `python scripts/evaluate_metrics.py --labels results/metrics/auto_labels_run1.jsonl --prompts prompts/prompts_refusal_benchmark.jsonl --out results/metrics/aggregate_run1.csv`
   4. `python scripts/perturbation_analysis.py --labels results/metrics/auto_labels_run1.jsonl --prompts prompts/prompts_refusal_benchmark.jsonl --out results/metrics/diagnostics_run1.csv`
   5. `python scripts/visualize_results.py --metrics results/metrics/aggregate_run1.csv --diagnostics results/metrics/diagnostics_run1.csv --figdir results/figures/run1`

4. **Human Evaluation**
   - Follow `docs/human_eval_protocol.md`; store labels in `results/human_labels.jsonl`.
   - Re-run `evaluate_metrics.py` with the `--human-labels` flag to merge signals.

5. **Versioning**
   - Tag releases with semantic versions (e.g., `v0.1-baseline`).
   - Record changes to prompts/taxonomy in CHANGELOG (future work).
