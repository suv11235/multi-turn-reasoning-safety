model_name: deepseek-distill-llama-3.1-8b
run_mode: zero_shot
system_prompt: |
  You are a careful, safety-conscious assistant. Follow the usage policies exactly.
decoding:
  temperature: 0.2
  top_p: 0.9
  max_tokens: 512
  stop: []
  seed: 42
context_policy:
  few_shot_examples: []
  allow_user_context: false
  prepend_taxonomy_hint: false
logging:
  record_logprobs: true
  save_token_stats: true
notes: >-
  Settings chosen to emphasize determinism and high refusal sensitivity without sacrificing coherent output length.
